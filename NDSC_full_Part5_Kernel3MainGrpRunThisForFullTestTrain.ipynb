{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# To follow Part 1 and Part 2 on these 3 main categories\n",
    "# split by image path to the 3 main categories : Beauty, fashion, mobile\n",
    "# categorise into Beauty Fashion and Mobile via information from image path\n",
    "# make into modules for easy usage\n",
    "import pandas as pd\n",
    "\n",
    "df = pd.read_csv('train.csv') # import training set\n",
    "\n",
    "image_path = df['image_path']\n",
    "df_beautyAll = df[image_path.str.contains('beauty_image', regex=True)]\n",
    "df_fashionAll = df[image_path.str.contains('fashion_image', regex=True)]\n",
    "df_mobileAll = df[image_path.str.contains('mobile_image', regex=True)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# manual change typeDataset ### attention!\n",
    "typeDataset= 'beauty'\n",
    "# typeDataset = 'fashion'\n",
    "#typeDataset = 'mobile'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# take 80% of each df_xxx and set as training while 20% of the training set is to test our accuracy of model\n",
    "lenDfBeauty = len(df_beautyAll)\n",
    "lenDfBeauty = round(0.001*(lenDfBeauty))  # take 80% as training set for classification\n",
    "df_beauty = df_beautyAll[0:lenDfBeauty]\n",
    "\n",
    "lenDfFashion = len(df_fashionAll)\n",
    "lenDfFashion = round(0.001*(lenDfFashion))  # take 80% as training set for classification\n",
    "df_fashion = df_fashionAll[0:lenDfFashion]\n",
    "\n",
    "lenDfMobile = len(df_mobileAll)\n",
    "lenDfMobile = round(0.001*(lenDfMobile))  # take 80% as training set for classification\n",
    "df_mobile = df_mobileAll[0:lenDfMobile]\n",
    "\n",
    "# from Part 6\n",
    "# take next 20% of each df_xxx and set as test set for trained model while first 80% was the training set for classification\n",
    "df_trainVerify_beauty = df_beautyAll[lenDfBeauty:round(2*lenDfBeauty)]\n",
    "df_trainVerify_fashion = df_fashionAll[lenDfFashion:round(2*lenDfFashion)]\n",
    "df_trainVerify_mobile = df_mobileAll[lenDfMobile:round(2*lenDfMobile)]\n",
    "\n",
    "print(df_trainVerify_mobile)\n",
    "print(len(df_trainVerify_mobile))\n",
    "print(df_trainVerify_mobile.iloc[len(df_trainVerify_mobile)-1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise to sentences # can skip since each cell is alr a sentence\n",
    "# for the 3 grps perform one by one, saving output as we go along\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "if typeDataset == 'beauty':\n",
    "    df = df_beauty\n",
    "elif typeDataset == 'fashion':\n",
    "    df = df_fashion\n",
    "elif typeDataset == 'mobile':\n",
    "    df = df_mobile\n",
    "\n",
    "print(len(df_beauty))\n",
    "print(len(df_fashion))\n",
    "print(len(df_mobile))\n",
    "\n",
    "print(len(df['title']))\n",
    "\n",
    "\n",
    "sents = []\n",
    "for text in df['title']:\n",
    "    sents.append(sent_tokenize(text))\n",
    "\n",
    "#print(len(sents))\n",
    "    \n",
    "#print(len(sents))\n",
    "#print(type(sents[0]))\n",
    "#print(sents)\n",
    "\n",
    "#sentsUnlist = list(chain(*sents)) # unlist a list of lists   #################3 alert replace for all copies\n",
    "#print(\"sents unlisted :{}\".format(sents))\n",
    "#print(sents)\n",
    "\n",
    "sentsUnlist = [' '.join(item) for item in sents]\n",
    "\n",
    "print(len(sents))\n",
    "print(len(sentsUnlist))\n",
    "\n",
    "#for item in sents:\n",
    "#    print(' '.join(item))\n",
    "#print('------------------------------')\n",
    "#for item in sentsUnlist:\n",
    "#    print(item)\n",
    "      \n",
    "sents = sentsUnlist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6 Tokenise to sentence\n",
    "# tokenise to sentences # can skip since each cell is alr a sentence\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from itertools import chain\n",
    "\n",
    "\n",
    "if typeDataset == 'beauty':\n",
    "    dftrainVerify = df_trainVerify_beauty \n",
    "elif typeDataset == 'fashion':\n",
    "    dftrainVerify = df_trainVerify_fashion \n",
    "elif typeDataset == 'mobile':\n",
    "    dftrainVerify = df_trainVerify_mobile\n",
    "\n",
    "\n",
    "sents1 = []\n",
    "for text in dftrainVerify['title']:\n",
    "    sents1.append(sent_tokenize(text))\n",
    "    \n",
    "\n",
    "\n",
    "sentsUnlist1 = [' '.join(item) for item in sents1]\n",
    "\n",
    "print(len(sents1))\n",
    "print(len(sentsUnlist1))\n",
    "\n",
    "#for item in sents:\n",
    "#    print(' '.join(item))\n",
    "#print('------------------------------')\n",
    "#for item in sentsUnlist:\n",
    "#    print(item)\n",
    "      \n",
    "sents1 = sentsUnlist1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenise into words\n",
    "from itertools import chain\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "#print(sents)\n",
    "words = [word_tokenize(sent) for sent in sents]\n",
    "print(len(sents))\n",
    "print(len(words))\n",
    "print(len(df))\n",
    "#print(words)\n",
    "df = df.assign(tokenised=words)\n",
    "# unlist the list\n",
    "words = list(chain(*words)) # unlist a list of lists\n",
    "#print(words)\n",
    "#print(len(words))\n",
    "#print(words[0:10])\n",
    "#print(df[0:10])\n",
    "\n",
    "\n",
    "#print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 6 Tokenise into words\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "#print(sents1)\n",
    "words1 = [word_tokenize(sent) for sent in sents1]\n",
    "#print(words)\n",
    "dftrainVerify = dftrainVerify.assign(tokenised=words1)\n",
    "# unlist the list\n",
    "#words1 = list(chain(*words1)) # unlist a list of lists\n",
    "#print(words)\n",
    "#print(len(words))\n",
    "#print(words[0:10])\n",
    "#print(df[0:10])\n",
    "\n",
    "\n",
    "#print(dftrainVerify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use variable 'words'\n",
    "# checking for bigrams and sort according to frequency and filter out those freq more than 2\n",
    "# be sure to save as diff file name for the 3 categories, under pickle save\n",
    "import nltk\n",
    "from nltk.collocations import *\n",
    "import pickle\n",
    "import os\n",
    "bigram_measures = nltk.collocations.BigramAssocMeasures()\n",
    "finder = BigramCollocationFinder.from_words(words)\n",
    "#finder = BigramCollocationFinder.from_words(list(df['removeStopwords']))\n",
    "bigrams = finder.ngram_fd.items()\n",
    "sorted(bigrams)\n",
    "\n",
    "\n",
    "\n",
    "# sort in descending order by frequency\n",
    "# Append to a list\n",
    "# https://stackoverflow.com/questions/28077573/python-appending-to-a-pickled-list \n",
    "biscores = []\n",
    "for k,v in sorted(bigrams, key=lambda t:t[-1], reverse=True):\n",
    "    #print(k,v)\n",
    "    if v > 1:\n",
    "        #print(k)\n",
    "        biTog = k[0]+\" \"+k[1]\n",
    "        #print((biTog))\n",
    "        biscores.append(biTog)\n",
    "\n",
    "\n",
    "print(len(biscores))\n",
    "#print(biTog)\n",
    "\n",
    "\n",
    "# Now we \"sync\" our database\n",
    "#with open(bigram_filename,'wb') as wfp:\n",
    "#    pickle.dump(biscores, wfp)\n",
    "\n",
    "# Re-load our database\n",
    "#with open(bigram_filename,'rb') as rfp:\n",
    "#    biscores = pickle.load(rfp)\n",
    "#print(len(biscores))\n",
    "#print(biscores)\n",
    "\n",
    "# we need to group these words together so they make sense\n",
    "\n",
    "# SAVE as a data file\n",
    "# https://stackoverflow.com/questions/899103/writing-a-list-to-a-file-with-python/899199\n",
    "# to save as json, can see\n",
    "# https://stackoverflow.com/questions/890485/python-how-do-i-write-a-list-to-file-and-then-pull-it-back-into-memory-dict-re\n",
    "# to automate, we can change the file name we save as by determining the 'typeDataset' in cell above\n",
    "if typeDataset == 'beauty':\n",
    "    saveNameBi = 'bigram_beauty_words.dat'\n",
    "elif typeDataset == 'fashion':\n",
    "    saveNameBi = 'bigram_fashion_words.dat'\n",
    "elif typeDataset == 'mobile':\n",
    "    saveNameBi = 'bigram_mobile_words.dat'\n",
    "\n",
    "# to write to a file called bigram_words.dat\n",
    "with open(saveNameBi, 'wb') as fp:\n",
    "        pickle.dump(biscores, fp)\n",
    "        #print(biscores)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# use variable 'words'\n",
    "# checking for trigrams and filter out if freq of trigram is more than 2\n",
    "# be sure to save as diff file name for the 3 categories, under pickle save\n",
    "# run all 3 grps from the top to this cell, before proceeding to next cell\n",
    "from nltk.collocations import *\n",
    "import pickle\n",
    "import os\n",
    "import nltk\n",
    "\n",
    "trigram_measures = nltk.collocations.TrigramAssocMeasures()\n",
    "finder = TrigramCollocationFinder.from_words(words)\n",
    "trigrams = finder.ngram_fd.items()\n",
    "sorted(trigrams)\n",
    "\n",
    "\n",
    "# sort in descending order by frequency\n",
    "# Append to a list\n",
    "# https://stackoverflow.com/questions/28077573/python-appending-to-a-pickled-list \n",
    "\n",
    "\n",
    "triscores = []\n",
    "for k,v in sorted(trigrams, key=lambda t:t[-1], reverse=True):\n",
    "    #print(k,v)\n",
    "    if v > 1:\n",
    "        #print(k)\n",
    "        triTog = k[0]+\" \"+k[1]+\" \" +k[2]\n",
    "        #print((triTog))\n",
    "        triscores.append(triTog)\n",
    "\n",
    "\n",
    "print(len(triscores))\n",
    "#print(triTog)\n",
    "\n",
    "# to automate, we can change the file name we save as by determining the 'typeDataset' in cell above\n",
    "if typeDataset == 'beauty':\n",
    "    saveNameTri = 'trigram_beauty_words.dat'\n",
    "elif typeDataset == 'fashion':\n",
    "    saveNameTri = 'trigram_fashion_words.dat'\n",
    "elif typeDataset == 'mobile':\n",
    "    saveNameTri = 'trigram_mobile_words.dat'\n",
    "\n",
    "with open(saveNameTri, 'wb') as fp:\n",
    "        pickle.dump(triscores, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the function to remove bi and trigrams from df['title']\n",
    "import removeBiTri3Grp\n",
    "# run this one grp at a time, as removeBiTri3Grp output only runs through one set of biscores and triscores at a time\n",
    "lenDf = len(df)\n",
    "#print(biscores)\n",
    "\n",
    "# pre allocate list\n",
    "# https://stackoverflow.com/questions/311775/python-create-a-list-with-initial-capacity\n",
    "listMonogram = [None] * lenDf\n",
    "listBothBiTri = [None] * lenDf\n",
    "print(len(df))\n",
    "for i in range(0, lenDf):\n",
    "    if i % 1000 == 0:\n",
    "        print('i:{}'.format(i))\n",
    "    #itemDescription = df['removeStopwords'][i]\n",
    "    itemDescription = df['title'].iloc[i]\n",
    "    #print(\"original: \\n{}\".format(itemDescription))\n",
    "    \n",
    "    #removebothBiTri, listTriBiThrown  = removeBiTri(itemDescription) # when itemDescription is already split into list\n",
    "    removebothBiTri, listTriBiThrown = removeBiTri3Grp.removeBiTri3Grp(itemDescription.split(' '), typeDataset) # when itemDescription is a string\n",
    "    #print(removebothBiTri, listTriBiThrown)\n",
    "    listMonogram[i] = removebothBiTri\n",
    "    listBothBiTri[i] = listTriBiThrown\n",
    "\n",
    "#print(df)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = df.assign(monogram = listMonogram)\n",
    "df = df.assign(bothBiTri = listBothBiTri)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# part 6\n",
    "# now use the previous Bi and Trigram words created to check if these words are inside\n",
    "# run this one grp at a time, as removeBiTri3Grp output only runs through one set of biscores and triscores at a time\n",
    "#https://stackoverflow.com/questions/311775/python-create-a-list-with-initial-capacity\n",
    "import removeBiTri3Grp \n",
    "\n",
    "lenDftrainVerify = len(dftrainVerify)\n",
    "#print(biscores)\n",
    "listMonogram1 = [None] * lenDftrainVerify\n",
    "listBothBiTri1 = [None] * lenDftrainVerify\n",
    "#print(dftrainVerify)\n",
    "print(lenDftrainVerify)\n",
    "for i in range(0, lenDftrainVerify):\n",
    "    # print to see if running\n",
    "    if i % 1000 == 0:\n",
    "        print('i:{}'.format(i))\n",
    "    #itemDescription = df['removeStopwords'][i]\n",
    "    itemDescription = dftrainVerify['title'].iloc[i]\n",
    "    #print(\"original: \\n{}\".format(itemDescription))\n",
    "    \n",
    "    #removebothBiTri, listTriBiThrown  = removeBiTri(itemDescription) # when itemDescription is already split into list\n",
    "    removebothBiTri, listTriBiThrown = removeBiTri3Grp.removeBiTri3Grp(itemDescription.split(' '), typeDataset) # when itemDescription is a string\n",
    "    #print(removebothBiTri, listTriBiThrown)\n",
    "    listMonogram1[i] = removebothBiTri\n",
    "    listBothBiTri1[i] = listTriBiThrown\n",
    "#print(dftrainVerify)\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6\n",
    "# idea is to create a list with initial value then add to it\n",
    "# faster than append to the whole df\n",
    "dftrainVerify = dftrainVerify.assign(monogram = listMonogram1)\n",
    "dftrainVerify = dftrainVerify.assign(bothBiTri = listBothBiTri1)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# dont run\n",
    "# using the function to remove bi and trigrams from df['title']\n",
    "import removeBiTri3Grp\n",
    "# run this one grp at a time, as removeBiTri3Grp output only runs through one set of biscores and triscores at a time\n",
    "lenDf = len(df)\n",
    "#print(biscores)\n",
    "df['monogram'] = \"\"\n",
    "df['bothBiTri'] = \"\"\n",
    "#print(df)\n",
    "for i in range(0, lenDf):\n",
    "    if i % 1000 == 0:\n",
    "        print('i:{}'.format(i))\n",
    "    #itemDescription = df['removeStopwords'][i]\n",
    "    itemDescription = df['title'].iloc[i]\n",
    "    #print(\"original: \\n{}\".format(itemDescription))\n",
    "    \n",
    "    #removebothBiTri, listTriBiThrown  = removeBiTri(itemDescription) # when itemDescription is already split into list\n",
    "    removebothBiTri, listTriBiThrown = removeBiTri3Grp.removeBiTri3Grp(itemDescription.split(' '), typeDataset) # when itemDescription is a string\n",
    "    #print(removebothBiTri, listTriBiThrown)\n",
    "    df['monogram'].iloc[i] = removebothBiTri\n",
    "    df['bothBiTri'].iloc[i] = listTriBiThrown\n",
    "\n",
    "#print(df)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words after removed bi and trigrams\n",
    "from nltk.corpus import stopwords \n",
    "from string import punctuation\n",
    "from itertools import chain\n",
    "import pickle\n",
    "from stop_words import get_stop_words\n",
    "# the 2nd stop_words library contains indonesian, the first did not\n",
    "print('here')\n",
    "customStopWords=set(stopwords.words('english')+list(punctuation) + get_stop_words('indonesian')+ get_stop_words('english'))\n",
    "print('here')\n",
    "# remove stop words\n",
    "\n",
    "wordsWOStopwords = []\n",
    "for text in df['monogram']:\n",
    "    text = ' '.join(text)\n",
    "    #print(text)\n",
    "    wordsWOStopwords.append([word for word in word_tokenize(text.lower()) if word not in customStopWords])\n",
    "print('here')\n",
    "df = df.assign(removeStopwords=wordsWOStopwords)\n",
    "#del df['tokenised']\n",
    "#print(df)\n",
    "# unlist the list\n",
    "wordsWOStopwords = list(chain(*wordsWOStopwords)) # unlist a list of lists\n",
    "#print(\"wordsWOStopwords unlisted :{}\".format(wordsWOStopwords))\n",
    "print('here')\n",
    "\n",
    "# export as a file of words without stop words, this is to save a variable\n",
    "#with open('noStopwords_words.dat', 'wb') as fp:\n",
    "#    pickle.dump(wordsWOStopwords, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# remove stop words after removed bi and trigrams\n",
    "from nltk.corpus import stopwords \n",
    "from string import punctuation\n",
    "from itertools import chain\n",
    "import pickle\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from stop_words import get_stop_words\n",
    "# the 2nd stop_words library contains indonesian, the first did not\n",
    "\n",
    "customStopWords=set(stopwords.words('english')+list(punctuation)+ get_stop_words('indonesian'))\n",
    "\n",
    "# remove stop words\n",
    "\n",
    "wordsWOStopwords = []\n",
    "for text in dftrainVerify['monogram']:\n",
    "    text = ' '.join(text)\n",
    "    #print(text)\n",
    "    wordsWOStopwords.append([word for word in word_tokenize(text.lower()) if word not in customStopWords])\n",
    "\n",
    "dftrainVerify = dftrainVerify.assign(removeStopwords=wordsWOStopwords)\n",
    "#del df['tokenised']\n",
    "#print(dftrainVerify)\n",
    "# unlist the list\n",
    "wordsWOStopwords = list(chain(*wordsWOStopwords)) # unlist a list of lists\n",
    "#print(\"wordsWOStopwords unlisted :{}\".format(wordsWOStopwords))\n",
    "\n",
    "\n",
    "# export as a file of words without stop words, this is to save a variable\n",
    "#with open('noStopwords_words.dat', 'wb') as fp:\n",
    "#    pickle.dump(wordsWOStopwords, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# write df to pickle\n",
    "# to automate, we can change the file name we save as by determining the 'typeDataset' in cell above\n",
    "import pickle\n",
    "if typeDataset == 'beauty':\n",
    "    fnremoveBiTriStop = 'df_removeTriBiStopBeauty.dat'\n",
    "elif typeDataset == 'fashion':\n",
    "    fnremoveBiTriStop = 'df_removeTriBiStopFashion.dat'\n",
    "elif typeDataset == 'mobile':\n",
    "    fnremoveBiTriStop = 'df_removeTriBiStopMobile.dat'\n",
    "    \n",
    "with open(fnremoveBiTriStop, 'wb') as fp:\n",
    "    pickle.dump(df, fp)\n",
    "print(df)\n",
    "\n",
    "# part 1 ends here"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# open back the file just saved\n",
    "import pickle\n",
    "if typeDataset == 'beauty':\n",
    "    fnremoveBiTriStop = 'df_removeTriBiStopBeauty.dat'\n",
    "elif typeDataset == 'fashion':\n",
    "    fnremoveBiTriStop = 'df_removeTriBiStopFashion.dat'\n",
    "elif typeDataset == 'mobile':\n",
    "    fnremoveBiTriStop = 'df_removeTriBiStopMobile.dat'\n",
    "    \n",
    "with open(fnremoveBiTriStop, 'rb') as fp:\n",
    "    df = pickle.load(fp)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# stemming the column \"removeStopwwords\" which is the final column after removing Stopwords and Bi Trigrams\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st=LancasterStemmer()\n",
    "\n",
    "\n",
    "stemmingWord = []\n",
    "for sentence in df['removeStopwords']:\n",
    "    #print()\n",
    "    #print(sentence)\n",
    "    sentence = ' '.join(sentence)\n",
    "    #print(sentence)\n",
    "    stemmedWords=[st.stem(word) for word in word_tokenize(sentence)]\n",
    "    #print(stemmedWords)\n",
    "    stemmingWord.append([stem for stem in stemmedWords])\n",
    "df = df.assign(stemWords=stemmingWord)\n",
    "print(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6\n",
    "# stemming the column \"removeStopwwords\" which is the final column after removing Stopwords and Bi Trigrams\n",
    "import nltk\n",
    "from nltk.tokenize import word_tokenize, sent_tokenize\n",
    "from nltk.stem.lancaster import LancasterStemmer\n",
    "st=LancasterStemmer()\n",
    "\n",
    "\n",
    "stemmingWord1 = []\n",
    "for sentence1 in dftrainVerify['removeStopwords']:\n",
    "    #print()\n",
    "    #print(sentence1)\n",
    "    sentence1 = ' '.join(sentence1)\n",
    "    #print(sentence1)\n",
    "    stemmedWords1=[st.stem(word) for word in word_tokenize(sentence1)]\n",
    "    #print(stemmedWords)\n",
    "    stemmingWord1.append([stem for stem in stemmedWords1])\n",
    "dftrainVerify = dftrainVerify.assign(stemWords=stemmingWord1)\n",
    "print(dftrainVerify)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# collate all words and put in 'mergeAll' column\n",
    "import pickle\n",
    "\n",
    "# make words from bigram and trigram to one word each\n",
    "# using the function to remove bi and trigrams from df['title']\n",
    "lenDf = len(df)\n",
    "listMergeAll = []\n",
    "for i in range(0, lenDf):\n",
    "    thisRowMono = df['stemWords'].iloc[i]\n",
    "    thisRowBiTri = df['bothBiTri'].iloc[i]\n",
    "    RowMonoStr = ' '.join(thisRowMono)\n",
    "    BitriNoSpace = ' '.join(thisRowBiTri)\n",
    "    mergestr = \"{} {}\".format(BitriNoSpace,RowMonoStr)\n",
    "    listMergeAll.append(mergestr)\n",
    "\n",
    "df = df.assign(mergeAll = listMergeAll)\n",
    "        \n",
    "\n",
    "#print(df)\n",
    "     \n",
    "# write df to pickle\n",
    "# to automate, we can change the file name we save as by determining the 'typeDataset' in cell above\n",
    "\n",
    "if typeDataset == 'beauty':\n",
    "    fnMergeAll = 'df_beauty_mergeAll.dat'\n",
    "elif typeDataset == 'fashion':\n",
    "    fnMergeAll = 'df_fashion_mergeAll.dat'\n",
    "elif typeDataset == 'mobile':\n",
    "    fnMergeAll = 'df_mobile_mergeAll.dat'\n",
    "    \n",
    "# writing\n",
    "with open(fnMergeAll, 'wb') as fp:\n",
    "    pickle.dump(df, fp)\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part 6\n",
    "# collate all words and put in 'mergeAll' column\n",
    "\n",
    "# make words from bigram and trigram to one word each\n",
    "# using the function to remove bi and trigrams from df['title']\n",
    "lenDftrainVerify = len(dftrainVerify)\n",
    "listMergeAll = []\n",
    "for i in range(0, lenDftrainVerify):\n",
    "    thisRowMono = dftrainVerify['stemWords'].iloc[i]\n",
    "    thisRowBiTri = dftrainVerify['bothBiTri'].iloc[i]\n",
    "    RowMonoStr = ' '.join(thisRowMono)\n",
    "    BitriNoSpace = ' '.join(thisRowBiTri)\n",
    "    mergestr = \"{} {}\".format(BitriNoSpace,RowMonoStr)\n",
    "    listMergeAll.append(mergestr)\n",
    "\n",
    "dftrainVerify = dftrainVerify.assign(mergeAll = listMergeAll)\n",
    "        \n",
    "        \n",
    "\n",
    "#print(dftrainVerify)\n",
    "    \n",
    "# pickle save \n",
    "# write df to pickle\n",
    "\n",
    "if typeDataset == 'beauty':\n",
    "    fnMergeAllVerify = 'dftrainVerify_mergeAll_TrainVerify_beauty.dat'\n",
    "elif typeDataset == 'fashion':\n",
    "    fnMergeAllVerify = 'dftrainVerify_mergeAll_TrainVerify_fashion.dat'\n",
    "elif typeDataset == 'mobile':\n",
    "    fnMergeAllVerify = 'dftrainVerify_mergeAll_TrainVerify_mobile.dat'\n",
    "\n",
    "\n",
    "with open(fnMergeAllVerify, 'wb') as fp:\n",
    "    pickle.dump(dftrainVerify, fp)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# load back the pickle to check\n",
    "with open(fnMergeAll, 'rb') as fp:\n",
    "    df = pickle.load(fp)\n",
    "#print(df)\n",
    "\n",
    "# since there are 3 groups, we should use a automated process to determine open which file\n",
    "if typeDataset == 'beauty':\n",
    "    fnMergeAllVerify = 'dftrainVerify_mergeAll_TrainVerify_beauty.dat'\n",
    "elif typeDataset == 'fashion':\n",
    "    fnMergeAllVerify = 'dftrainVerify_mergeAll_TrainVerify_fashion.dat'\n",
    "elif typeDataset == 'mobile':\n",
    "    fnMergeAllVerify = 'dftrainVerify_mergeAll_TrainVerify_mobile.dat'\n",
    "\n",
    "    \n",
    "\n",
    "# Load the pickle of the test training set in Part 6\n",
    "with open(fnMergeAllVerify, 'rb') as fp:\n",
    "    dftrainVerify = pickle.load(fp)\n",
    "print(dftrainVerify)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# use this as the Pipeline model for classification Naives Bayes rule\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "text_clf = Pipeline([\n",
    "    ('vect', CountVectorizer()),\n",
    "    ('tfidf',TfidfTransformer()),\n",
    "    ('clf', MultinomialNB()),\n",
    "])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "##rb\n",
    "\n",
    "# Alternative,  testing with another classifier model \n",
    "# result showed this model has higher accuracy rate, so we go with this classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.kernel_approximation import AdditiveChi2Sampler\n",
    "from sklearn.kernel_approximation import RBFSampler\n",
    "from sklearn.kernel_approximation import SkewedChi2Sampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "\n",
    "\n",
    "#X_features = chi2_feature.fit_transform(df['mergeAll'], df['Category'])\n",
    "#clf = SGDClassifier(max_iter=10, tol=1e-3)\n",
    "#clf.fit(X_features, df['Category'])\n",
    "\n",
    "# to make the y for Kernel Approximation\n",
    "if typeDataset== 'beauty':\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('chi2sampler', AdditiveChi2Sampler(sample_steps=2)),  # likely also perform fit_transform on this\n",
    "        #('rbf_feature', RBFSampler(gamma=1, random_state=1)),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-4, random_state=42,\n",
    "                              max_iter=5, tol=None)),\n",
    "    ])\n",
    "\n",
    "if typeDataset== 'fashion':\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1,3))),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('chi2sampler', AdditiveChi2Sampler(sample_steps=2)),  # likely also perform fit_transform on this\n",
    "        #('rbf_feature', RBFSampler(gamma=1, random_state=1)),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-2, random_state=42,\n",
    "                              max_iter=5, tol=None)),\n",
    "    ])\n",
    "if typeDataset== 'mobile':\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('chi2sampler', AdditiveChi2Sampler(sample_steps=2)),  # likely also perform fit_transform on this\n",
    "        #('rbf_feature', RBFSampler(gamma=1, random_state=1)),# worst one\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-4, random_state=42,\n",
    "                              max_iter=5, tol=None)),\n",
    "    ])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# fitting into the pipeline to train\n",
    "#clf.fit(X_features, y)\n",
    "X = df['mergeAll']\n",
    "y=df['Category']\n",
    "text_clf.fit(X, y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle\n",
    "\n",
    "if typeDataset == 'beauty':\n",
    "    fnMergeAllVerify = 'dftrainVerify_mergeAll_TrainVerify_beauty.dat'\n",
    "elif typeDataset == 'fashion':\n",
    "    fnMergeAllVerify = 'dftrainVerify_mergeAll_TrainVerify_fashion.dat'\n",
    "elif typeDataset == 'mobile':\n",
    "    fnMergeAllVerify = 'dftrainVerify_mergeAll_TrainVerify_mobile.dat'\n",
    "\n",
    "\n",
    "with open(fnMergeAllVerify, 'rb') as fp:\n",
    "    dftrainVerify = pickle.load(fp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# using the pipeline on a training data to predict\n",
    "import numpy as np\n",
    "predicted =  text_clf.predict(dftrainVerify['mergeAll'])\n",
    "\n",
    "dftrainVerify = dftrainVerify.assign(predicted=predicted)\n",
    "\n",
    "if typeDataset == 'beauty':\n",
    "    fntext= 'beautyTraining.csv'\n",
    "elif typeDataset == 'fashion':\n",
    "    fntext= 'fashionTraining.csv'\n",
    "elif typeDataset == 'mobile':\n",
    "    fntext= 'mobileTraining.csv'\n",
    "dftrainVerify.to_csv(fntext)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Evaluation on performance of test\n",
    "# this part if you use \"test\", need to upload to kaggle to see result\n",
    "import numpy as np\n",
    "from sklearn import metrics\n",
    "\n",
    "# predicted is done above\n",
    "print(np.mean(predicted ==  dftrainVerify.Category))\n",
    "\n",
    "\n",
    "print(metrics.classification_report(dftrainVerify.Category, predicted))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning on traning set\n",
    "# https://scikit-learn.org/stable/tutorial/text_analytics/working_with_text_data.html\n",
    "\n",
    "from sklearn.model_selection import GridSearchCV\n",
    "parameters = {\n",
    "    'vect__ngram_range': [(1, 1), (1, 2),(1,3),(1,4),(1,5)],\n",
    "    #'tfidf__use_idf': (True, False),\n",
    "    'clf__alpha': (1e-5,1e-4, 1e-3,1e-2),\n",
    "}\n",
    "gs_clf = GridSearchCV(text_clf, parameters, cv=5, iid=False, n_jobs=-1)\n",
    "\n",
    "#gs_clf = gs_clf.fit(df['mergeAll'][:400], df['Category'][:400])\n",
    "gs_clf = gs_clf.fit(df['mergeAll'], df['Category'])\n",
    "#print(df['mergeAll'][:400])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tuning on training set\n",
    "#a = gs_clf.predict(['4gb32gbblack'])\n",
    "print(gs_clf.best_score_)\n",
    "for param_name in sorted(parameters.keys()):\n",
    "    print(\"%s: %r\" % (param_name, gs_clf.best_params_[param_name]))"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# testing alert!!!! dun use\n",
    "X_1 = []\n",
    "X_2 = [] \n",
    "for i in range(0,len(dftrainVerify['mergeAll'])):\n",
    "    X_1.append(dftrainVerify['mergeAll'].iloc[i])\n",
    "    X_2.append(dftrainVerify['Category'].iloc[i])\n",
    "print(len(X_1))\n",
    "print(len(X_2))\n",
    "type(X_1)\n",
    "print(list(zip(X_1,X_2)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X= zip([dftrainVerify['mergeAll'].iloc[i],dftrainVerify['Category'].iloc[i]])\n",
    "print(X)"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# Alternative,  testing with another classifier model \n",
    "# result showed this model has higher accuracy rate, so we go with this classifier\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.kernel_approximation import SkewedChi2Sampler\n",
    "from sklearn.linear_model import SGDClassifier\n",
    "from sklearn.feature_extraction.text import TfidfTransformer\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "# to make the X for Kernel Approximation\n",
    "#X_1 = []\n",
    "#X_2 = [] \n",
    "#for i in range(0,len(dftrainVerify['mergeAll'])):\n",
    "#    X_1.append(df['mergeAll'].iloc[i])\n",
    "#    X_2.append(df['Category'].iloc[i])\n",
    "#print(len(X_1))\n",
    "#print(len(X_2))\n",
    "#type(X_1)\n",
    "#zipped = zip(X_1,X_2)\n",
    "#listZipped = [list(a) for a in  zipped]\n",
    "#print(listZipped)\n",
    "\n",
    "# to make the y for Kernel Approximation\n",
    "#y = [] \n",
    "#for i in range(0,len(dftrainVerify['mergeAll'])):\n",
    "#    y.append(dftrainVerify['mergeAll'].iloc[i])\n",
    "#print(y)\n",
    "\n",
    "#X = [[0, 0], [1, 1], [1, 0], [0, 1]]\n",
    "#y = [0, 0, 1, 1]\n",
    "\n",
    "\n",
    "X_features = chi2_feature.fit_transform(X, y)\n",
    "print(X_features)\n",
    "#clf = SGDClassifier(max_iter=10, tol=1e-3)\n",
    "#clf.fit(X_features, y)\n",
    "\n",
    "clf = SGDClassifier(max_iter=10, tol=1e-3)\n",
    "\n",
    "\n",
    "# to make the y for Kernel Approximation\n",
    "if typeDataset== 'beauty':\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1,2))),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-4, random_state=42,\n",
    "                              max_iter=5, tol=None)),\n",
    "        ('chi2_feature', SkewedChi2Sampler(skewedness=.01,\n",
    "                                  n_components=10,\n",
    "                                 random_state=0))\n",
    "    ])\n",
    "if typeDataset== 'fashion':\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1,4))),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-4, random_state=42,\n",
    "                              max_iter=5, tol=None)),\n",
    "    ])\n",
    "if typeDataset== 'mobile':\n",
    "    text_clf = Pipeline([\n",
    "        ('vect', CountVectorizer(ngram_range=(1,1))),\n",
    "        ('tfidf',TfidfTransformer()),\n",
    "        ('clf', SGDClassifier(loss='hinge', penalty='l2',\n",
    "                           alpha=1e-4, random_state=42,\n",
    "                              max_iter=5, tol=None)),\n",
    "    ])\n",
    "chi2_feature = SkewedChi2Sampler(skewedness=.01,\n",
    "                                  n_components=10,\n",
    "                                 random_state=0)\n",
    "\n",
    "X_features = chi2_feature.fit_transform(X, y)\n",
    "print(X_features)\n",
    "#clf = SGDClassifier(max_iter=10, tol=1e-3)\n",
    "#clf.fit(X_features, y)\n",
    "\n",
    "clf = SGDClassifier(max_iter=10, tol=1e-3)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
